{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import random\n",
    "random.seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/horse-tracking-data'\n",
    "derby_path = 'data/big-data-derby-2022'\n",
    "# List to store filenames and paths\n",
    "def get_files(folder_path):\n",
    "    file_list = []\n",
    "    race_list = []\n",
    "    # Iterate through the folder\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "                full_path = os.path.join(root, file)\n",
    "                file_list.append(full_path)\n",
    "                race_list.append(file.split('.')[0])\n",
    "\n",
    "    # print(file_list)\n",
    "    # print(race_list)\n",
    "    return file_list, race_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_file_split(file_list):\n",
    "    random.shuffle(file_list)\n",
    "\n",
    "    split_index = int(0.8 * len(file_list))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "\n",
    "    print(f\"Total Files : {len(file_list)}\")\n",
    "    print(f\"Number of training files: {len(train_files)}\")\n",
    "    print(f\"Number of testing files: {len(test_files)}\")\n",
    "\n",
    "    return train_files, test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataframes for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for train and test\n",
    "def get_dataframes(train_files, test_files):\n",
    "    train_df = []\n",
    "    test_df = []\n",
    "\n",
    "    # Load and combine training data\n",
    "    for file in tqdm(train_files):\n",
    "        data = pd.read_csv(file)  # Adjust the function if needed (e.g., read_parquet for other formats)\n",
    "        train_df.append(data)\n",
    "\n",
    "    train_df = pd.concat(train_df, ignore_index=True)\n",
    "\n",
    "    # Load and combine testing data\n",
    "    for file in tqdm(test_files):\n",
    "        data = pd.read_csv(file)\n",
    "        test_df.append(data)\n",
    "\n",
    "    test_df = pd.concat(test_df, ignore_index=True)\n",
    "\n",
    "    # Remove after race columns\n",
    "    train_df = train_df[train_df['is_race_going']]\n",
    "    test_df = test_df[test_df['is_race_going']]\n",
    "    \n",
    "\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Testing data shape: {test_df.shape}\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "       'cumulative_distance_travelled', 'position',\n",
    "\n",
    "       'distance_to_leader', 'speed_1s','acceleration_1s', \n",
    "       \n",
    "       'speed_1s_lag1', 'speed_1s_lag2', 'speed_1s_lag3', \n",
    "       \n",
    "       'acceleration_1s_lag1','acceleration_1s_lag2', 'acceleration_1s_lag3',\n",
    "\n",
    "       'distance_to_leader_lag1', 'distance_to_leader_lag2', 'distance_to_leader_lag3',\n",
    "      \n",
    "       'remaining_distance', 'leader_remaining_distance']\n",
    "\n",
    "target = ['target_variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_splits(train_df, test_df):\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df[target]\n",
    "\n",
    "    X_test = test_df[feature_columns]\n",
    "    y_test = test_df[target]\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "def scale_data(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Normalize\n",
    "    X_train_scaled = scaler.fit_transform(X_train)  # NumPy array\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "    y_train['target_variable'] = y_train['target_variable'].apply(lambda x : 0 if x<0 else 0.999999 if x>=1 else x)\n",
    "    y_test['target_variable'] = y_test['target_variable'].apply(lambda x : 0 if x<0 else 0.999999 if x>=1 else x)\n",
    "\n",
    "    # temporary trial\n",
    "    # multiply t_train target_variable and t_test target_variable by 100\n",
    "    y_train['target_variable'] *= 100\n",
    "    y_test['target_variable'] *= 100\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "        'position', #'cumulative_distance_travelled',\n",
    "\n",
    "        'distance_to_leader',\n",
    "         'speed_1s','acceleration_1s', \n",
    "       \n",
    "       # 'speed_1s_lag1', 'speed_1s_lag2', 'speed_1s_lag3', \n",
    "       \n",
    "       # 'acceleration_1s_lag1','acceleration_1s_lag2', 'acceleration_1s_lag3',\n",
    "\n",
    "       # 'distance_to_leader_lag1', 'distance_to_leader_lag2', 'distance_to_leader_lag3',\n",
    "        # 'remaining_distance',\n",
    "        'leader_remaining_distance'\n",
    "        ] \n",
    "        \n",
    "        # 'curve'] # from the updated column\n",
    "\n",
    "target = ['target_variable']\n",
    "def fetch_clean_data(folder_path):\n",
    "    file_list, _ = get_files(folder_path)\n",
    "    train_files, test_files = train_test_file_split(file_list)\n",
    "    train_df, test_df = get_dataframes(train_files, test_files)\n",
    "    X_train, X_test, y_train, y_test = get_X_y_splits(train_df, test_df)\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test , scalar= scale_data(X_train, X_test, y_train, y_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_it(model, X_train_scaled, y_train):\n",
    "    model.fit(X_train_scaled,y_train)\n",
    "    return model\n",
    "\n",
    "def predict_this(model, X):\n",
    "    x_pred = pd.DataFrame()\n",
    "    x_pred['pred'] = model.predict(X)\n",
    "    return x_pred['pred'].apply(lambda x : 0 if x<0 else 99.9999 if x>=100 else x)\n",
    "\n",
    "def get_rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_arc(model, X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    model = train_it(model, X_train_scaled, y_train)\n",
    "    y_train_pred = predict_this(model, X_train_scaled)\n",
    "    y_test_pred = predict_this(model, X_test_scaled)\n",
    "\n",
    "    # rmse\n",
    "    train_rmse = get_rmse(y_train['target_variable'], y_train_pred)\n",
    "    test_rmse = get_rmse(y_test['target_variable'], y_test_pred)\n",
    "    print(\"Train RMSE: \", train_rmse)\n",
    "    print(\"Test RMSE: \", test_rmse)\n",
    "\n",
    "    return model, train_rmse, test_rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = fetch_clean_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(n_estimators=100, learning_rate=0.01, random_state=42, max_depth = 5)\n",
    "# model = CatBoostRegressor(n_estimators=100, learning_rate=0.1, loss_function='rmse', depth= 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do bernoulli here\n",
    "\n",
    "\n",
    "\n",
    "def plotter(win_prob, rank_, remain_, race_name, save_folder):\n",
    "\n",
    "    base_name = os.path.splitext(race_name)[0]  # Removes the file extension\n",
    "    file_name = f\"{base_name}.png\"\n",
    "    file_name = file_name.split(\"/\")[-1]\n",
    "\n",
    "    save_path = os.path.join(save_folder, file_name)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharex=True, sharey=False)\n",
    "\n",
    "    df_win = win_prob.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='win_probability')\n",
    "\n",
    "    # df_win['smooth_win_probability'] = df_win.groupby('program_number')['win_probability'].transform(\n",
    "    # lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "    # )\n",
    "    # Plot 1: Win Probabilities\n",
    "    sns.lineplot(\n",
    "        data=df_win, \n",
    "        x='trakus_index', \n",
    "        y='win_probability', \n",
    "        hue='program_number', \n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(\"Win Probabilities Over Time\")\n",
    "    axes[0].set_xlabel(\"Time (Trakus Index)\")\n",
    "    axes[0].set_ylabel(\"Win Probability\")\n",
    "    axes[0].legend(title=\"Program Number\")\n",
    "\n",
    "    df_rank = rank_.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='position')\n",
    "\n",
    "    # Plot 2: Positions\n",
    "    sns.lineplot(\n",
    "        data=df_rank, \n",
    "        x='trakus_index', \n",
    "        y='position', \n",
    "        hue='program_number', \n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"Positions Over Time\")\n",
    "    axes[1].set_xlabel(\"Time (Trakus Index)\")\n",
    "    axes[1].set_ylabel(\"Position\")\n",
    "    axes[1].legend(title=\"Program Number\")\n",
    "\n",
    "\n",
    "    df_remaining_dist = remain_.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='distance_to_leader')\n",
    "    # Plot 3: Distance to Leader\n",
    "    sns.lineplot(\n",
    "        data=df_remaining_dist, \n",
    "        x='trakus_index', \n",
    "        y='distance_to_leader', \n",
    "        hue='program_number', \n",
    "        ax=axes[2]\n",
    "    )\n",
    "    axes[2].set_title(\"Distance to Leader Over Time\")\n",
    "    axes[2].set_xlabel(\"Time (Trakus Index)\")\n",
    "    axes[2].set_ylabel(\"Distance to Leader\")\n",
    "    axes[2].legend(title=\"Program Number\")\n",
    "\n",
    "    fig.suptitle(race_name)\n",
    "\n",
    "    # Adjust layout\n",
    "    # plt.tight_layout()\n",
    "    # print(\"Saving image to : \", save_path)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "def monte_carlo_horse_race_simulation(remain_, pred_, rank_):\n",
    "    # Number of Monte Carlo simulations\n",
    "    n_simulations = 100\n",
    "    size = 100  # Number of simulations per iteration\n",
    "\n",
    "    win_prob = pd.DataFrame(index=pred_.index, columns=pred_.columns)\n",
    "    error_count = 0\n",
    "    # Monte Carlo simulation loop\n",
    "    for sim in range(n_simulations):\n",
    "        # Temporary DataFrame for storing simulated ranks in this iteration\n",
    "        cur_win_prob = pd.DataFrame(index=range(size), columns=pred_.columns)\n",
    "        \n",
    "        # Simulate for each race\n",
    "        for i, row in pred_.iterrows():\n",
    "            for fp in pred_.columns:\n",
    "                # Skip if horse has finished\n",
    "                if remain_.loc[i][fp] < 0:\n",
    "                    cur_win_prob[fp] = [1] * size\n",
    "                    continue\n",
    "                \n",
    "                # Poisson distribution for rank differences\n",
    "                lambda_ = abs(remain_.loc[i][fp] * row[fp])  # Expected value\n",
    "                \n",
    "                # Two-step Monte Carlo simulation\n",
    "                # 1. Poisson distribution for rank differences\n",
    "                try:\n",
    "                    fp_rank_diff = np.random.poisson(lambda_, size)\n",
    "                except Exception as e:\n",
    "                    error_count +=1\n",
    "                # 2. Calculate predicted ranks ÃŸ(First to finish is the winner)\n",
    "                pred_rank = rank_.loc[i][fp] - fp_rank_diff\n",
    "                \n",
    "                # Store simulation results\n",
    "                cur_win_prob[fp] = pred_rank\n",
    "        \n",
    "            # Aggregate win probabilities for this simulation\n",
    "            for fp in pred_.columns:\n",
    "                # Calculate win probability for this Monte Carlo iteration\n",
    "                if sim == 0:\n",
    "                    # Initialize on first iteration\n",
    "                    win_prob.loc[i, fp] = sum(cur_win_prob[fp] == 1) / size\n",
    "                else:\n",
    "                    # Accumulate probabilities\n",
    "                    win_prob.loc[i, fp] += sum(cur_win_prob[fp] == 1) / size\n",
    "\n",
    "    # Normalize probabilities across all horses\n",
    "    win_prob = win_prob.divide(n_simulations)\n",
    "    win_prob = win_prob.divide(win_prob.sum(axis=1), axis=0)    \n",
    "  \n",
    "    return win_prob\n",
    "\n",
    "def monte_carlo_wrapper(race_data):\n",
    "    # the reace I cwill get here will be after passing it to model already\n",
    "    race_data.sort_values('trakus_index', inplace=True)\n",
    "    race_data = race_data.reset_index()\n",
    "\n",
    "    # convert to string representation\n",
    "    race_data['program_number'] = race_data['program_number'].astype(str).str.strip()\n",
    "\n",
    "    # brenoulli now\n",
    "    size = 1000\n",
    "    rank_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='position')\n",
    "    remain_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='leader_remaining_distance')\n",
    "    pred_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='target_variable')\n",
    "\n",
    "    win_prob = monte_carlo_horse_race_simulation(remain_, pred_, rank_)\n",
    "    return win_prob, rank_\n",
    "\n",
    "def bernoulli_race(race_data):\n",
    "    # dataframe containing one race worth of data\n",
    "    # for each trakus index, calculate prob for each hors\n",
    "    # return dataframe\n",
    "    \n",
    "    # the reace I cwill get here will be after passing it to model already\n",
    "    race_data.sort_values('trakus_index', inplace=True)\n",
    "    race_data = race_data.reset_index()\n",
    "\n",
    "    # convert to string representation\n",
    "    race_data['program_number'] = race_data['program_number'].astype(str).str.strip()\n",
    "\n",
    "    # brenoulli now\n",
    "    size = 1000\n",
    "    rank_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='position')\n",
    "    remain_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='leader_remaining_distance')\n",
    "    pred_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='target_variable')\n",
    "\n",
    "    win_prob = pred_.copy() \n",
    "\n",
    "    total_rows = win_prob.shape[0]\n",
    "    error_rows = 0\n",
    "\n",
    "    for i, row in pred_.iterrows():\n",
    "        cur_win_prob = pd.DataFrame(index=range(size))\n",
    "        for fp in pred_.columns:\n",
    "            if remain_.loc[i][fp] < 0:\n",
    "                cur_win_prob[fp] = [1]*1000\n",
    "                continue\n",
    "            try:\n",
    "                fp_rank_diff = np.random.binomial(remain_.loc[i][fp], row[fp], size)\n",
    "            except Exception as e:\n",
    "                # print(\"Error occurred during Bernoulli race calculation for race. , error : \" + str(e))\n",
    "                # print current variable values\n",
    "                # print(f\"Status: Rem : {remain_.loc[i][fp]} || Target: {row[fp]}\")\n",
    "                error_rows += 1\n",
    "                \n",
    "            pred_rank = rank_.loc[i][fp] - fp_rank_diff\n",
    "            cur_win_prob[fp] = pred_rank\n",
    "        # cur_win_prob = cur_win_prob.rank(method='min', axis=1)\n",
    "        for fp in pred_.columns:\n",
    "            win_prob.loc[i, fp] = sum(cur_win_prob[fp]==1) / size\n",
    "\n",
    "    # norm\n",
    "    win_prob = win_prob.divide(win_prob.sum(axis=1), axis=0)\n",
    "\n",
    "    # if error_rows > 0:\n",
    "        # print(f\"Error occurred {error_rows} times during Bernoulli race calculation. ({error_rows/total_rows} %)\")\n",
    "\n",
    "    return win_prob, rank_\n",
    "\n",
    "\n",
    "\n",
    "def bernoulli_super(file_list, model, scalar,save_folder):\n",
    "    # multiple files\n",
    "    # read each file, pass through model to predict target\n",
    "    # take target df for the race, pass to bernoulli_race to get probabilities for that, store. \n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    for file in tqdm(file_list):\n",
    "        # print(\"For Race : \" + str(file))\n",
    "        race_data = pd.read_csv(file)\n",
    "        race_data = race_data[race_data['is_race_going']]\n",
    "        race_features = scalar.transform(race_data[feature_columns])\n",
    "        race_data['target_variable'] = model.predict(race_features)\n",
    "        race_data['target_variable'] = race_data['target_variable'].apply(lambda x : 0 if x<0 else 99.9999 if x>=100 else x)\n",
    "        race_data['target_variable'] = race_data['target_variable']/100\n",
    "        # pass race_data to bernoulli race function\n",
    "        win_prob, rank_ = bernoulli_race(race_data)\n",
    "        # win_prob, rank_ = monte_carlo_wrapper(race_data)\n",
    "\n",
    "        remain_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='distance_to_leader')\n",
    "\n",
    "        # plot probabilities for that race\n",
    "        plotter(win_prob, rank_, remain_, file, save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(folder_path):\n",
    "    # takes in folder path, extracts data,\n",
    "    # trains model\n",
    "    # creates probability function\n",
    "\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test , scalar= fetch_clean_data(folder_path)\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.01, random_state=42, max_depth = 5)\n",
    "    model, _, _ = training_arc(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "    return model, scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,scalar = pipeline(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test , scalar= fetch_clean_data(folder_path)\n",
    "\n",
    "\n",
    "# get average value of y_train['target_variable'] and get std dev\n",
    "\n",
    "\n",
    "y_train.mean(), y_train.std(), y_test.mean(), y_test.std()\n",
    "\n",
    "# get average value of y_test['target_variable'] and get std dev\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list, race_list = get_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list)\n",
    "train_set, test_set= train_test_file_split(file_list)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_super(test_set, model, scalar,save_folder=\"output-plots/orig-5-features-xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acceleration_1s</th>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance_to_leader</th>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speed_1s</th>\n",
       "      <td>540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position</th>\n",
       "      <td>873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leader_remaining_distance</th>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            score\n",
       "acceleration_1s              32.0\n",
       "distance_to_leader          429.0\n",
       "speed_1s                    540.0\n",
       "position                    873.0\n",
       "leader_remaining_distance  1197.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_important = model.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "f_imp_df = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by=\"score\", ascending=True)\n",
    "f_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m file_list[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_list' is not defined"
     ]
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_list)\n",
    "indx = df[df[0]== 'data/horse-tracking-data/AQU_2019-01-20_1.csv'].index\n",
    "indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data = pd.read_csv(file_list[0])\n",
    "race_data = race_data[race_data['is_race_going']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(race_data.sort_values('trakus_index'))[race_data['trakus_index']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data[feature_columns].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_features = scalar.transform(race_data[feature_columns])\n",
    "# race_features\n",
    "\n",
    "race_data['target_variable'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_data['target_variable'] = model.predict(race_features)\n",
    "race_data['target_variable'] = race_data['target_variable'].apply(lambda x : 0 if x<0 else 0.999999 if x>=1 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data.sort_values('trakus_index', inplace=True)\n",
    "race_data = race_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data['program_number'] = race_data['program_number'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli\n",
    "size = 1000\n",
    "rank_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='position')\n",
    "remain_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='leader_remaining_distance')\n",
    "pred_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='target_variable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_prob = pred_.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "for i, row in pred_.iterrows():\n",
    "    cur_win_prob = pd.DataFrame(index=range(size))\n",
    "    for fp in pred_.columns:\n",
    "        if remain_.loc[i][fp] < 0:\n",
    "            cur_win_prob[fp] = [1]*1000\n",
    "            continue\n",
    "        fp_rank_diff = np.random.binomial(remain_.loc[i][fp], row[fp], size)\n",
    "        pred_rank = rank_.loc[i][fp] - fp_rank_diff\n",
    "        cur_win_prob[fp] = pred_rank\n",
    "    # cur_win_prob = cur_win_prob.rank(method='min', axis=1)\n",
    "    for fp in pred_.columns:\n",
    "        win_prob.loc[i, fp] = sum(cur_win_prob[fp]==1) / size\n",
    "\n",
    "# norm\n",
    "win_prob = win_prob.divide(win_prob.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_prob[\"total_prob\"] = win_prob.sum(axis=1)\n",
    "win_prob[\"total_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_prob.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for Plotly\n",
    "df_melted = win_prob.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='win_probability')\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(df_melted, \n",
    "              x='trakus_index', \n",
    "              y='win_probability', \n",
    "              color='program_number', \n",
    "              title='Win Probabilities Over Time',\n",
    "              labels={'trakus_index': 'Time (Trakus Index)', 'win_probability': 'Win Probability'},\n",
    "              template='plotly_dark')  # Optional: Use a dark theme\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title_font_size=20,\n",
    "    legend_title_text='Program Number',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for Plotly\n",
    "df_melted = win_prob.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='win_probability')\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(df_melted, \n",
    "              x='trakus_index', \n",
    "              y='win_probability', \n",
    "              color='program_number', \n",
    "              title='Win Probabilities Over Time',\n",
    "              labels={'trakus_index': 'Time (Trakus Index)', 'win_probability': 'Win Probability'},\n",
    "              template='plotly_dark')  # Optional: Use a dark theme\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title_font_size=20,\n",
    "    legend_title_text='Program Number',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for Plotly\n",
    "df_melted = win_prob.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='win_probability')\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(df_melted, \n",
    "              x='trakus_index', \n",
    "              y='win_probability', \n",
    "              color='program_number', \n",
    "              title='Win Probabilities Over Time',\n",
    "              labels={'trakus_index': 'Time (Trakus Index)', 'win_probability': 'Win Probability'},\n",
    "              template='plotly_dark')  # Optional: Use a dark theme\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title_font_size=20,\n",
    "    legend_title_text='Program Number',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for Plotly\n",
    "df_melted = rank_.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='position')\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(df_melted, \n",
    "              x='trakus_index', \n",
    "              y='position', \n",
    "              color='program_number', \n",
    "              title='Positions Over Time',\n",
    "              labels={'trakus_index': 'Time (Trakus Index)', 'position': 'Position'},\n",
    "              template='plotly_dark')  # Optional: Use a dark theme\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title_font_size=20,\n",
    "    legend_title_text='Program Number',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remain_ = pd.pivot_table(race_data, index='trakus_index', columns='program_number', values='distance_to_leader')\n",
    "df_melted = remain_.reset_index().melt(id_vars='trakus_index', \n",
    "                                  var_name='program_number', \n",
    "                                  value_name='distance_to_leader')\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(df_melted, \n",
    "              x='trakus_index', \n",
    "              y='distance_to_leader', \n",
    "              color='program_number', \n",
    "              title='Distance to Leader over time',\n",
    "              labels={'trakus_index': 'Time (Trakus Index)', 'distance_to_leader': 'Distance to Leader'},\n",
    "              template='plotly_dark')  # Optional: Use a dark theme\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title_font_size=20,\n",
    "    legend_title_text='Program Number',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1, 10, 11, 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSFEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
